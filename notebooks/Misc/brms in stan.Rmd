---
title: "BRMS in Stan Analysis"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
Given over 10,000 observations and 1000 unique UPC's, this model will take a while to compile on a local machine. Thus, an R Studio AMI was created on AWS, with the instance running on 16 cores and many gigs of ram. While the AMI contained RStan 2.18 (current version as of 2/1/20 is 2.19.2), it did not contain the RStanARM or BRMS packages (and those packages could not be installed due to technical complications). 

Fortunately, BRMS contains functions to translate the high-level LME4 syntax to a Stan script, as well as translating the data.frame object to a list object. 

```{r}
#Data object
stan_data <- make_standata(price_norm ~ (1|region_id/banner_id) - 1 + store_id + UPC, 
                          data = model_data)
save(stan_data,  file = "data/brms_data.rda")
```

```{r}
#stan script
make_stancode(price_norm ~ (1|region_id/banner_id) - 1 + store_id + UPC, 
                  data = model_data, #note- NOT stan_data
                  family = gaussian(),
                  prior = prior1,
                  save_model = "models/bmrs_model.stan")
```

Both the Stan script and list object were uploaded to the AWS Instance. From there, using RStan, the Stan script was compiled to C++ and the sampling function was used to execute the model. Despite the computational power provided by AWS, the model still took about 12 hours to sample using 4 chains and 1000 iterations each.

```{r}
m1 <- stanc(file = "models/bmrs_model.stan")
m1 <- stan_model(stanc_ret = m1)
fit1 <- sampling(m1, #use sampling function, rather than stan()
                 warmup=250, #
                 iter=1000, #total iterations including warmup
                 seed=1, #ensures reproducibility 
                 data=stan_data, 
                 chains = 4, 
                 cores = getOption("mc.cores", 1L),
                 verbose = FALSE,
                 refresh = 1)
```

Once the model was executed, global environment was saved as an RData file and this file was downloaded to my local machine using Filezilla. For more details on how this is done, see my article here.

```{r}
df1 <- data.frame(stan = parnames(fit1))
df2 <- data.frame(bmrs = parnames(brms_model))
df3 <- data.frame(bmrs = rep(0, (1062-1041)))
df4 <- rbind(df2, df3)
df5 <- cbind(df1, df4)
```

WOW this shit is messy. Variable names lost in translation?

Here we go.

For "b", there are 1016 parameters. I'm guessing the first 17 are for the store_id because they have much larger n_eff sizes. 

Compare prior to posterior.

```{r}
options(max.print=10000)
```

```{r}
summary(fit1, pars = c('sigma', 'sd_1', 'z_1', 'sd_2', 'z_2', 'r_1_1', 'r_2_1'), 
        probs = c(0.025, 0.975),
        digits = 2)$summary


```

```{r}
prior_summary(fit1)
```

```{r}
plot(fit1, N = 2, ask = TRUE)
```

```{r}
plot(conditional_effects(fit1), points = TRUE) 
```

```{r}
pp_check(m1)
```

```{r}
loo(m1)
```


```{r}
names(m1)
```

```{r}
options(max.print=100000)
sims <- as.matrix(m1)
dim(sims)
colnames(sims)
```


```{r}
posterior_interval(sims, prob = 0.95, pars = 'region_idNewYork')
apply(sims, 2, median)
```


```{r}
summary(residuals(m1)) # not deviance residuals
```

```{r}
cov2cor(vcov(m1))
```

```{r}
launch_shinystan(fit1)
```

```{r}
y <- as.numeric(model_data$price_norm)
y_rep <- posterior_predict(m1)
```

