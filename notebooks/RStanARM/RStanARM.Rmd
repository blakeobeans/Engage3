---
title: "RStanARM"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r, message = FALSE, warning = FALSE}
library(rstan)
library(rstanarm)
library(loo)
library(tidyverse)
library(bayesplot)
library(shinystan)
library(here)
library(lme4)
library(gridExtra)
setwd(here())
```

As always, prep RStan to run on multiple cores.

```{r}
rstan_options(auto_write = TRUE) #cache model
options(mc.cores = parallel::detectCores()) #multiple cores for multiple chains
```

I recently had the opportunity to interview with Engage3. Engage3 is a company that does...I'm not sure exactly. They do gather super-market level data, though, which is something that I have some experience in, having written my (undergrad thesis)[https://github.com/blakeobeans/website/blob/master/projects/Shurtz_Blake_Undergraduate_Thesis.pdf] on collusion in agricultural products using super-market data.  

Anyway, as part of the interview process, they sent me a data analysis coding challenge. While I did the original analysis in Python as a (Jupyter Notebook)[https://github.com/blakeobeans/Engage3/blob/master/misc/Shurtz_Blake_Engage3_Report.pdf], I had been mulling over the dataset since.  

The result is this vignette, which it really a full-on Bayesian workflow using the Engage3 data. For me, it was a chance to level-up my skills in Stan and related packages, as well as a chance to level-up my skills in multi-level modeling.

### The Data

Engage3 sends auditors into super-markets to gather prices on various items. The dataset contains prices for items (the regression response variable). For each item, along with the price, we have data on the UPC, the store_id, the supermarket (aka the "banner"- both terms will be used synonymously) and the region within the continental US. 

```{r}
model_data <- read.csv("data/model_data.csv")
colnames(model_data) <- c("Price", "store_id", "UPC", "banner_id", "region_id")
model_data$store_id <- as.factor(model_data$store_id)
model_data$UPC <- as.factor(model_data$UPC)
sample_n(model_data, 5)
```

There are four different regions: Kansas, NY, NorCal and Texas. There are 5 different supermarkets ("banner_id"): Whole Foods, Safeway, Walmart, Wegans and Trade Joes. There are 17 different stores and 1000 unique UPCs.

### The Questions

There were two questions on the application, first, which regions are more/less expensive and second, which super-market brands are more/less expensive? 

As a burgeoning data scientist, I have a lot of tools in my toolbelt to approach such a question. My first thought is that, because I'm trying to estimate model parameters, I should use a regression framework. 

The simplest model would be a linear regression of region, banner_id, store_id and UPC on prices. However, it is unlikely that this is the *best* model, given the structure of the data.

The data exhibits a hierarchical *structure*: UPC items are within different super-markets, which are themselves within different regions. Given the structure of the data, a better (ie. more accurate) model would likely be a multi-level model (MLM).  

Why? Typically when dealing with structure data like the Engage3 dataset, standard models overestimate the "fixed" effects of a given variable. For example, if New York had a higher premium on groceries, that premium would be higher with a standard model. Multi-level models "pull" this regional effect back towards the "grand mean" for all regions in a way that is optimal. Of course, this doesn't need to be assumed true at face value, and these models can be compared. In fact, we'll do this near the end of the vignette. 

### Exploratory Analysis

Even the optimal Box-Cox logrithmic transformation of the price variable (the dependent variable) gives a pretty skewed distribution, so we'll scale the data in order to sample efficiency. Thus, the beta coefficients will have a "standard deviation" interpretation, and we'll use the gaussian family in the regression model.

```{r}
qplot(scale(model_data$Price))
model_data$price_norm = scale(model_data$Price)
```

How many of each stores are within each super-market within each region? Dplyr to the rescure.

```{r}
model_data %>% group_by(region_id, banner_id) %>% 
  summarise(n = sum(length(unique(store_id))))
```

There are only 4/5 super-markets in each region except for Texas. The UPC data comes from one super-market in each region.

There is an old joke in Freedmans "Hidden Order" about two supermarkets advertising that they have the lowest prices. This is only true as long as they have the lowest prices on different goods. So, in comparing super-markets, we need to compare them on the basis of the same items. There are 1000 unique super-market items in the dataset.  

```{r}
model_data_spread <- model_data %>% select(-Price) %>% spread(key = UPC, value = price_norm)
d <- nrow(model_data_spread) - colSums(is.na(model_data_spread))
quickplot(d) + ggtitle("Frequency Distribution- Number of Items per Store")
```

Typically, each item appears in 10 out of 17 stores on average.

### The Model



#### Why Bayes?

The model will be executed in a Bayesian framework using Stan. The justification for Bayesian methods in this case is the limited number of super-markets in each region. There is at most one super-market in each region, making estimation of the "super-market effect" difficult. Prior distributions allow us to model the initial distribution of each nested effect prior to looking at the data.  

```{r lme4 model}
m_lme <- lme4::lmer(formula = price_norm ~ (1|region_id/banner_id) - 1 + UPC,
              data = model_data)

```


#### Package Selection

Initially, this project was coded in Stan.[link here] As the project progressed, most advisors in the Stan forums recommended either the BRMS package or RStanARM. Both packages utilie Stan for sampling but present use the familiar LME4 package syntax for coding multi-level models.

BRMS and RStanARM are more similar than different. Initially, BRMS was chosen for its ability eport raw Stan code and Stan data objects. However, RStanARM allows the eport of the model's Shiny object to ShinyApps.io, which allows permanent hosting of the ShinyStan model.


#### Prior Selection 

Priors on the regression coefficients are all standard normal. These are regularizing priors.

Recall that the dependent variable is scaled with an approximate range between -2 and 2. Furthermore, all predictors are factors. Technically, the model is a mixed effects ANOVA, but we're interested in interpreting regression coefficients. 

Thus, given the range of scaled prices, the a prioir effect of a certain level of any factor is unlikely to be more than a single standard deviation, and this is before statistically adjusting for the effect of the item itself, which will likely explain most of the variation in price (the relative cost of milk to a potato is much more likely to be explained by the production costs of the item itself than the region or super-market that the item is sold in.)

Exponential priors for for the model variance are used.

#### Fixed vs. Random Effects and Nesting

The nesting structure is pretty straightforward. At the store level, we have the UPC and the store_id. The next level is the super-market, which is nested within a given region by being modeled as an interaction effect. In the author's opinion it is a negelected point to mention that the nesting component of a multilevel is separate from the partial pooling component, with the former being represented in the model as an interaction and the latter being represented as each level having a shared prior distribution (in a Bayesian framework). The nesting structure allows us to ask, for example, does a Safeway and Norcal charge more than a Safeway in Kansas?

The LME4 syntax emphasizes fixed vs. random effects, whose definitions are confusing at best and mis-leading at worst. Allow me to provide my own interpretation for assigning variables to one of each effect category.

When it comes to deciding whether or not a variable qualifies as a fixed or random effect, there are many sources to help guide one's thinking, but the heuristic that I have found most useful is to ask: does it make sense to have an 'average level' for the parameter? This gets back to the notion of "partial pooling" and the Bayesian perspective of having shared hyper-parameters for the prior distribution. Let's apply this heuristic to each factor.

For region_id, it makes sense to have an "average level." If you were to average all the regions in the US, and their average is something like the US itself. 

For banner_id, does it make sense to have an "average super-market?" Again, yes it does. Although there is no "average supermarket" per se, Safeway feels something like an average super-market, with there being higher and lower-end substitutes. 

For store_id, does it make sense to have an average store? I would lean towards no. While there is conceptually an average super-market, there is no average store. Once you account for the brand, they're pretty much all the same- a Safeway is a Safeway is a Safeway. In fact, because there is only one banner_id for each store, it doesn't make sense to condition on the store.

For UPC- does it make sense to have an average item? Absolutely not. You can't average milk and a potato. (Potato milkshake, anyone?)

Thus, the model is written below:

```{r}
m1 <- stan_glmer(formula = price_norm ~ (1|region_id/banner_id) + UPC - 1,
              data = model_data, 
              family = gaussian,
              prior = normal(location = 0, scale = 1, autoscale = FALSE),
              prior_intercept = normal(location = 0, scale = 1, autoscale = FALSE),
              prior_aux = exponential(rate = 1, autoscale = FALSE),
              cores = 4,
              chains = 4,
              iter = 1000,
              warmup = 250,
              seed = 1, 
              refresh = 1,
              control = list(max_treedepth = 15,
                             adapt_delta = 0.95))
```


## Diagnostics

### ShinyStan

The first, and easiest, way to perform diagnostics is to do so in (ShinyStan)http://mc-stan.org/shinystan/articles/deploy_shinystan.html], the Shiny app that provides common diagnostics and estimates on your executed Stan model.  

On advantage of the RStanARM app over BRMS is that you can upload your Stan model to (ShinyApps.io)[link] for permanent storage. Otherwise, the app only deploys locally. One caveat is that the free version of ShinyApps only supports Apps that are up to (1 GB in size)[https://community.rstudio.com/t/unable-to-deploy-shiny-app-in-shinyapps-io-the-application-failed-to-start-exited-with-code-137/18787]. Surprisingly this Stan model exceeds that size (this is mainly due to having over 10k observations on 4 chains with 1000 iterations each). So, I've uploaded a smaller version of the model (4 chains, but with fewer iterations) available (here)[https://blakeobeans.shinyapps.io/ShinyStan_Engage3/].

```{r shinystan}
y <- as.numeric(model_data$price_norm) #for posterior pred check
y_rep <- posterior_predict(m1) # for posterior pred check
launch_shinystan(m1) #launch it
###to save to shinyapps.io
sso <- as.shinystan(m1) #convert to shiny object
deploy_shinystan(sso, "ShinyStan_Engage3", account = 'blakeobeans') #deploy
```

### MCMC Diagnostics

There are two sorts of diagnostics to perform: general MCMC diagnostics and then diagnostics that are unique to the sampling algorithm. We'll take the latter first.

```{r, Betancourt's overview}
source('scripts/stan_utility_rstanarm.R')
check_div(m1)
check_rhat(m1)
check_n_eff(m1) 
check_energy(m1)
check_treedepth(m1)
```

#### NUTS diagnostics

```{r diagnostics setup}
l_post <- log_posterior(m1)
nuts_p <- nuts_params(m1)

names(m1$coefficients) #identify parameters of interest
pars = names(m1$coefficients[1001:1021])
```

##### Divergence

The best way to visualize divergence for multiple parameters simultaneously is to use mcmc_parcoord.

```{r}
color_scheme_set("darkgray")
mcmc_parcoord(m1, np = nuts_p, pars = pars) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

We can see from the divergence charts that the divergent samples were mostly from chain 2. We can see that the divergent chains failed to trace out the full shape of the log-posterior (top right).

```{r mcmc_nuts_divergence}
color_scheme_set("red")
mcmc_nuts_divergence(nuts_p, l_post, chain = 2)
```

##### Energy

For each chain, the two histograms should look the same. If not, you probably need to re-paramaterize. Chains 1 and 3 look the most similar.

```{r mcmc_nuts_energy}
color_scheme_set("red")
mcmc_nuts_energy(nuts_p)
```

#### General MCMC Diagnostics

You can use the summary table as a basis to create other plots and tables.

```{r create summary tibble}
 m1_summary <- summary(m1) %>% 
  as.data.frame() %>% 
  mutate(variable = rownames(.)) %>% 
  select(variable, everything()) %>% 
  as_tibble()
```

##### R-hat

We can see that, for the region parameters, the r-hat did not adequately converge to 1. We have pretty good results for New York, but the rest are lacking. This means that we're getting different results in each chain. If I were submitting this work, I would run more iterations on my model. But for the purposes of this article, I will ignore the error.

```{r mcmc_rhat}
color_scheme_set("brightblue") # see help("color_scheme_set")
rhats <- rhat(m1, pars = pars) #works as a CDF too
mcmc_rhat(rhats) + yaxis_text(hjust = 1)
```

There are over 1000 parameters in the model, particularly all of the UPCs. The UPC's have only a few observations each. Thus, it is difficult to get exact estimates of the effect. This explains the high r-hats among the majority of the parameters.

```{r rhat histogram}
m1_summary %>% 
  ggplot(aes(Rhat)) + 
  geom_histogram() 
```

##### Effective Sample Size

The effective sample size is an estimate of the number of independent draws from the posterior distribution of the estimands of interest. The more the better, but you want most more than .1. Good to check with the autocorrelation plot (next).

```{r mcmc_neff}
ratios_cp <- neff_ratio(m1, pars = pars)
#print(ratios_cp)
mcmc_neff(ratios_cp, size = 2)
```

```{r neff histogram}
iterations = m1[["stanfit"]]@sim[["iter"]]
m1_summary %>% 
  ggplot(aes(n_eff/iterations)) + 
  geom_histogram() 
```

##### Autocorrelation

NUTS is much better than Gibbs at reducing autocorrelation. We can see that chain 3 does remarkably better than the others.

```{r mcmc_acf}
mcmc_acf(m1, pars = pars, lags = 10)
```


### Model Summary

First, I'll look at the results of the model and in particular, the variances. Then we'll answer the questions around comparing regions and supermarkets.

```{r stan model summary}
#str(m1, max = 1)
options(max.print = 10) #used to limit coefficient summaries
print(m1)
```

We can calculate the intraclass correlation coefficient at the regional level:

```{r icc}
.11^2/(.11^2 + .13^2 + .085^2) #regional variance
.13^2/(.11^2 + .13^2 + .085^2) #supermarket variance
.085^2/(.11^2 + .13^2 + .085^2) #residual variance
```

We can compare these to the lmer model.

```{r lmer model summary}
print(m_lme)
```

We can see that the estimates of the variance are similar between the LME4 model and the Stan model. 

We can look at the credibility intervals for the variance parameters. We can see that the LME4 estimates fall within the intervals. (Note the sigmas for the parameters are variances around the intercept, not the error term itself.)

```{r mcmc_plot sigmas}
pars= c("sigma", "Sigma[banner_id:region_id:(Intercept),(Intercept)]", "Sigma[region_id:(Intercept),(Intercept)]")
posterior_interval(m1, pars = pars)
```

A residual analysis reveals that both models provide almost identical range.

```{r}
summary(residuals(m1)) # not deviance residuals
summary(residuals(m_lme))
```

### Analysis of MCMC Draws

While the Bayesplot package offers quick "out of the box" visualization of MCMC draws, it's easy enough to extract draws and plot them in ggplot. (Just make sure you extract the right variables!)

Furthermore, chain 3 looked like it had the best estimates (no divergences, the least autocorrelation), so we'll draw from this chain only.

#### Region Effects

We can see that Norcal is the region with the highest prices.


```{r regions boxplots}
sims <- as.array(m1)
sims <- sims[,3,c(1018:1021)]
sims <- as.data.frame(sims)
colnames(sims)
colnames(sims) <- c("Kansas", "New York", "NorCal", "Texas")
regions <- gather(sims, colnames(sims), key = "region", value = "price")

###
ggplot(regions, aes(x = region, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
```


#### Supermarkets Nested within Regions

Indeed, there is quite a bit of variance of supermarkets within regions. Keep in mind these are a comparison of random effects.

```{r plot supermarkets within regions}
sims <- as.array(m1)
sims <- sims[,3,c(1001:1017)]
sims <- as.data.frame(sims)
colnames(sims) #17 different supermarkets
###safeway
safeway <- as.data.frame(sims)[,c(1, 2, 3)]
colnames(safeway) <- c("Kansas", "NorCal", "Texas")
safeway <- gather(safeway, colnames(safeway), key = "safeway", value = "price")
#tjs
tjs <- as.data.frame(sims)[,c(4, 5, 6, 7)]
colnames(tjs) <- c("Kansas", "New York", "NorCal", "Texas")
tjs <- gather(tjs, colnames(tjs), key = "tjs", value = "price")
###walmart
walmart <- as.data.frame(sims)[,c(8, 9, 10, 11)]
colnames(walmart) <- c("Kansas", "New York", "NorCal", "Texas")
walmart <- gather(walmart, colnames(walmart), key = "walmart", value = "price")
###Wegmans
Wegmans <- as.data.frame(sims)[,c(12, 13, 14)]
colnames(Wegmans) <- c("Kansas", "New York", "Texas")
Wegmans <- gather(Wegmans, colnames(Wegmans), key = "Wegmans", value = "price")
###wholefoods
wholefoods <- as.data.frame(sims)[,c(15, 16, 17)]
colnames(wholefoods) <- c("New York", "Norcal", "Texas")
wholefoods <- gather(wholefoods, colnames(wholefoods), key = "wholefoods", value = "price")

###
p1 <- ggplot(safeway, aes(x = safeway, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
p2 <- ggplot(tjs, aes(x = tjs, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
p3 <- ggplot(walmart, aes(x = walmart, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
p4 <- ggplot(Wegmans, aes(x = Wegmans, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
p5 <- ggplot(wholefoods, aes(x = wholefoods, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
grid.arrange(p1, p2, p3, p4, p5, ncol=1)
```

Pricing looks pretty consistent across regions- Whole Foods is the most expensive, Walmart is the least.

### Model Comparison with Loo

Given the consistent pricing across regions, one may ask- is it even worth nesting? We can build a model to test this assumption.

```{r non-nested model}
m2 <- stan_glmer(formula = price_norm ~ (1|region_id) + (1|banner_id) - 1 + UPC,
              data = model_data, 
              family = gaussian,
              prior = normal(location = 0, scale = 1, autoscale = FALSE),
              prior_intercept = normal(location = 0, scale = 1, autoscale = FALSE),
              prior_aux = exponential(rate = 1, autoscale = FALSE),
              cores = 4,
              chains = 2,
              iter = 500,
              warmup = 100,
              seed = 1, 
              refresh = 1)
```


Oh, and we can take this opportunity to compare the multi-level model to a standard regression. The prior in RStanARM is for how much of the variation is explained by the predictors.

```{r stan_lm}
m1_lm <- stan_lm(formula = price_norm ~ region_id + banner_id + UPC -1,
              data = model_data, 
              prior = R2(0.5),
              cores = 4,
              chains = 4,
              iter = 250,
              warmup = 50,
              seed = 1, 
              refresh = 1,
              control = list(max_treedepth = 15,
                             adapt_delta = 0.95))
```

Loo uses leave-one-out validation to compare the expected log density. That sounds complicated, but it's really not that different from RMSE using density estimates on the modeled statistical distributions.

There were some technical difficulties around using Loo- perhaps related to R Studio- but the waic comparison worked and show that the nested model has the lowest elpd score. In other words, the non-nested model fits better. 

```{r loo comparison}
m1_waic <- waic(m1, cores = 4)
m1_lm_waic <- waic(m1_lm, cores = 4)
m2_waic <- waic(m2, cores = 4)
###compare
m1_waic[["estimates"]]
m1_lm_waic[["estimates"]]
m2_waic[["estimates"]]
```


### Posterior Predictive Check

Using Bayesplot. We can see that our model doesn't fit well at the extremes.

```{r ppc}
pp_check(m1)
```

### Conclusions

A 3 level model offers the best results, with regions having a price premium of, and supermarkets having a price premium of...

```{r 3 level model}
sims <- as.matrix(m2)
pars <- colnames(sims)[c(1001:1009)] #17 different supermarkets
posterior_interval(m2, prob = 0.95, pars = pars)
```

```{r m2 boxplots}
sims <- as.matrix(m2)
sims <- sims[,c(1001:1009)]
sims <- as.data.frame(sims)
colnames(sims) <- c("Saveway", "TJs", "Walmart", "Wegmans", "Whole Foods", "Kansas", "New York", "NorCal", "Texas")
m2_df <- gather(sims, colnames(sims), key = "region_banner", value = "price")

###
ggplot(m2_df, aes(x = region_banner, y = price)) +
  geom_boxplot() +
  coord_flip() +
  ylim(-.25, .25)
```

